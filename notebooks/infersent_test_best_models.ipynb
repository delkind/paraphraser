{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "infersent_test_best_models",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/delkind/paraphraser/blob/master/notebooks/infersent_test_best_models.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "BlmQIFSLZDdc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## This short notebook loads the 2 best decoders models we trained, and the trained infersent embeddings, and show their result. \n",
        "\n",
        "\n",
        "Just open in colab, and run the whole notebook.  Allow it to run for 10 minutes to setup and download the models.\n"
      ]
    },
    {
      "metadata": {
        "id": "8KTZjzV-6oFH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Installs"
      ]
    },
    {
      "metadata": {
        "id": "Bb-EcilYDEh2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "c590bd60-b6b0-4530-b26e-41d445799546"
      },
      "cell_type": "code",
      "source": [
        "!pip install --quiet pydrive \n",
        "\n",
        "!rm -r paraphraser  #remove previous github copy if needed\n",
        "!git clone https://github.com/delkind/paraphraser.git\n",
        "import sys\n",
        "sys.path.append('paraphraser/src')\n",
        "\n",
        "\n",
        "!git clone https://github.com/philipperemy/keras-tcn.git\n",
        "%cd keras-tcn\n",
        "#pip install -r requirements.txt # change to tensorflow if you dont have a gpu.\n",
        "!pip install . --quiet --upgrade # install it as a package.\n",
        "%cd ..\n",
        "from tcn import tcn\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import requests\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    file_id = '1w0GYh65l8r21IHTT6kVvdTMKIg6Y_i1M'\n",
        "    destination = 'embeddings.h5'\n",
        "    download_file_from_google_drive(file_id, destination)\n",
        "\n",
        "                    "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'paraphraser'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 281 (delta 12), reused 19 (delta 5), pack-reused 248\u001b[K\n",
            "Receiving objects: 100% (281/281), 1.02 MiB | 2.50 MiB/s, done.\n",
            "Resolving deltas: 100% (149/149), done.\n",
            "fatal: destination path 'keras-tcn' already exists and is not an empty directory.\n",
            "/content/keras-tcn\n",
            "/content\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "74LwqpCH8NVg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Dataset  creation"
      ]
    },
    {
      "metadata": {
        "id": "jz8a7Czb8PkR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import codecs\n",
        "import csv\n",
        "import random\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import keras\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.data_utils import get_file\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "END_SYMBOL = '<end>'\n",
        "START_SYMBOL = '<start>'\n",
        "# SPEC_CHARS_REPLACEMENT = {'.': ' <DOT> ', ',': ' <COMMA> ', ';': ' <SEMICOLON> ', ':': ' <COLON> ', '!': ' <EXCL> ',\n",
        "#                           '?': ' <QSTN> ', \"'\": ' <QUOTE> '}\n",
        "# SPEC_CHARS_RECONSTRUCT = {' ' + v.strip(): k for (k, v) in SPEC_CHARS_REPLACEMENT.items()}\n",
        "\n",
        "MIN_FREQ = 15\n",
        "URL_ROOT = \"https://raw.githubusercontent.com/scrollmapper/bible_databases/master/csv/t_\"\n",
        "CSV_EXT = \".csv\"\n",
        "OOV = '<OOV>'\n",
        "MAX_SENTENCE_LENGTH = 60\n",
        "\n",
        "\n",
        "def multi_replace(string, replacements, ignore_case=False):\n",
        "    \"\"\"\n",
        "    Given a string and a dict, replaces occurrences of the dict keys found in the\n",
        "    string, with their corresponding values. The replacements will occur in \"one pass\",\n",
        "    i.e. there should be no clashes.\n",
        "    :param str string: string to perform replacements on\n",
        "    :param dict replacements: replacement dictionary {str_to_find: str_to_replace_with}\n",
        "    :param bool ignore_case: whether to ignore case when looking for matches\n",
        "    :rtype: str the replaced string\n",
        "    \"\"\"\n",
        "    rep_sorted = sorted(replacements, key=lambda s: len(s[0]), reverse=True)\n",
        "    rep_escaped = [re.escape(replacement) for replacement in rep_sorted]\n",
        "    pattern = re.compile(\"|\".join(rep_escaped), re.I if ignore_case else 0)\n",
        "    return pattern.sub(lambda match: replacements[match.group(0)], string)\n",
        "\n",
        "\n",
        "class Dataset:\n",
        "    def __init__(self):\n",
        "        self.word2index = {}\n",
        "        self.index2word = {}\n",
        "\n",
        "    @staticmethod\n",
        "    def end_symbol():\n",
        "        return END_SYMBOL\n",
        "\n",
        "    def create_mapping(self, corpora):\n",
        "        segs = [seg for corpus in corpora.values() for sentence in corpus for seg in sentence]\n",
        "        frequencies = Counter(segs).items()\n",
        "        oov = [c for (_, c) in frequencies if c <= MIN_FREQ]\n",
        "        frequencies = {k: l for (k, l) in frequencies if l > MIN_FREQ or k == OOV}\n",
        "        frequencies[OOV] = sum(oov)\n",
        "        markers = [END_SYMBOL ,START_SYMBOL ] + list(corpora.keys())\n",
        "        self.word2index = {seg: num for (num, seg) in enumerate(markers + sorted(list(frequencies.keys())))}\n",
        "        #for m in sorted(markers):\n",
        "        #    self.word2index[m] = len(self.word2index)\n",
        "        assert (self.word2index[END_SYMBOL]==0) # in embedding_layer, mask_true assumes padding=0\n",
        "        self.index2word = {v: k for k, v in self.word2index.items()}\n",
        "        \n",
        "\n",
        "    def index(self, corpora):\n",
        "        self.create_mapping(corpora)\n",
        "        oov = self.word2index[OOV]\n",
        "        return {key: [[self.word2index.get(seg, oov) for seg in sentence] for sentence in corpus]\n",
        "                for (key, corpus) in corpora.items()}\n",
        "\n",
        "    def normalize(self, sentence):\n",
        "        sentence = re.sub('[^ a-zA-Z0-9.,:;!\\?\\'{}]', ' ', sentence.lower())\n",
        "        return sentence.strip()\n",
        "\n",
        "    def build_embeddings(self, sentences, w2v_path):\n",
        "        word_dict = self.get_word_dict(sentences)\n",
        "        return self.get_w2v(word_dict, w2v_path)\n",
        "\n",
        "    def get_word_dict(self, sentences):\n",
        "        # create vocab of words\n",
        "        word_dict = {}\n",
        "        for sent in sentences:\n",
        "            for word in sent:\n",
        "                if word not in word_dict:\n",
        "                    word_dict[word] = ''\n",
        "        word_dict[self.word2index[START_SYMBOL]] = ''\n",
        "        word_dict[self.word2index[END_SYMBOL]] = ''\n",
        "        return word_dict\n",
        "\n",
        "    def get_w2v(self, word_dict, w2v_path):\n",
        "        # create word_vec with w2v vectors\n",
        "        word_vec = {}\n",
        "        with open(w2v_path, encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                word, vec = line.split(' ', 1)\n",
        "                if word in self.word2index:\n",
        "                    if self.word2index[word] in word_dict:\n",
        "                        word_vec[word] = np.fromstring(vec, sep=' ')\n",
        "                    else:\n",
        "                        print(\"Embedding not found for \" + self.word2index[word])\n",
        "        print('Found %s(/%s) words with w2v vectors' % (len(word_vec), len(word_dict)))\n",
        "        return word_vec\n",
        "\n",
        "\n",
        "def decorate_file(file):\n",
        "    return '<{}>'.format(file)\n",
        "\n",
        "\n",
        "class BibleDataset(Dataset):\n",
        "    def __init__(self, files, embeddings, base_url=URL_ROOT, suffix=CSV_EXT, test_split=0.1, validation_split=0.1, v2w_path=None):\n",
        "        super().__init__()\n",
        "        corpora, index = self.parse_csv(base_url, files, suffix)\n",
        "        self.corpora = self.index(corpora)\n",
        "        self.train, self.val, self.test = self.split(index, test_split, validation_split)\n",
        "        sentence_lengths = [len(s) for c in self.corpora.values() for s in c]\n",
        "        self.max_sentence_length = max(sentence_lengths)\n",
        "        self.style2index = {s: i for (i, s) in enumerate(corpora.keys())}\n",
        "        self.index2style = {v: k for k, v in self.style2index.items()}\n",
        "        with h5py.File(embeddings, \"r\") as ds:\n",
        "            self.embeddings = {decorate_file(file): np.array(ds[file]) for file in files if file in ds}\n",
        "        if v2w_path is not None:\n",
        "            sentences = []\n",
        "            for file in files:\n",
        "                sentences += self.corpora[decorate_file(file)]\n",
        "            self.word_vec = self.build_embeddings(sentences, v2w_path)\n",
        "        pass\n",
        "\n",
        "    def parse_csv(self, base_url, files, suffix):\n",
        "        corpora = {}\n",
        "        for file in files:\n",
        "            corpus = {}\n",
        "            with open(get_file(file, base_url + file + suffix, cache_dir='/tmp/bible.cache/'), \"rb\") as webfile:\n",
        "                for idx, row in enumerate(csv.reader(codecs.iterdecode(webfile, 'utf-8'))):\n",
        "                    if idx > 0:\n",
        "                        segs = [str(s).strip() for s in word_tokenize(self.normalize(row[4]))\n",
        "                                if len(str(s).strip()) > 0]\n",
        "\n",
        "                        if len(segs) > MAX_SENTENCE_LENGTH:\n",
        "                            segs = segs[:MAX_SENTENCE_LENGTH]\n",
        "                        corpus[tuple(int(v) for v in row[:-1])] = segs\n",
        "            corpora[decorate_file(file)] = corpus\n",
        "\n",
        "        keysets = [set(ks.keys()) for ks in corpora.values()]\n",
        "        intersection = [s for s in sorted(list(keysets[0].intersection(*keysets[1:])))\n",
        "                        if len([corp[s] for corp in corpora.values() if len(corp[s]) == 0]) == 0]\n",
        "\n",
        "        corpora = {file: [corpus[key] for key in intersection] for (file, corpus) in corpora.items()}\n",
        "        index = [key[1] for key in intersection]\n",
        "\n",
        "        return corpora, index\n",
        "\n",
        "    @staticmethod\n",
        "    def split(index, test_split, validation_split):\n",
        "        count_test = len(index) * test_split\n",
        "        count_validation = len(index) * validation_split\n",
        "        count = Counter(index)\n",
        "\n",
        "        sum_validation = 0\n",
        "        sum_test = 0\n",
        "\n",
        "        test = []\n",
        "        validation = []\n",
        "\n",
        "        for i in reversed(range(1, len(count) + 1)):\n",
        "            if sum_test < count_test:\n",
        "                sum_test += count[i]\n",
        "                test += [i]\n",
        "            elif sum_validation < count_validation:\n",
        "                sum_validation += count[i]\n",
        "                validation += [i]\n",
        "            else:\n",
        "                break\n",
        "        test = set(test)\n",
        "        validation = set(validation)\n",
        "        train = set(index) - test - validation\n",
        "\n",
        "        train = (0, sum([count[i] for i in train]))\n",
        "        validation = (train[1], train[1] + sum([count[i] for i in validation]))\n",
        "        test = (validation[1], validation[1] + sum([count[i] for i in test]))\n",
        "\n",
        "        return train, test, validation\n",
        "\n",
        "    def pad_sentence(self, sentence, length): \n",
        "        #note length can be small, creating truncation, len(sentence)=60 but length=15\n",
        "        return sentence[:length] + [self.word2index[END_SYMBOL]] * max(0,length - len(sentence))\n",
        "\n",
        "    def recostruct_sentence(self, sentence):\n",
        "        return ' '.join([self.index2word[seg] for seg in sentence])\n",
        "\n",
        "    def create_sequences(self, file, batch,max_sent_len=None,one_hot=True):\n",
        "        X1, X2, y = list(), list(), list()\n",
        "        # walk through each sentence in batch\n",
        "        max_size = max_sent_len if max_sent_len else self.max_sentence_length + 2\n",
        "        for num in batch:\n",
        "            seq = [self.word2index[START_SYMBOL]] + self.corpora[file][num]\n",
        "            # split one sequence into multiple X,y pairs\n",
        "            #for i in range(1, len(seq)):\n",
        "            i = len(seq)\n",
        "            # split into input and output pair\n",
        "            in_seq, out_seq = seq[:], seq[1:]\n",
        "            # pad input sequence\n",
        "            \n",
        "            in_seq = self.pad_sentence(in_seq, max_size)\n",
        "            out_seq=  self.pad_sentence(out_seq,max_size)\n",
        "            \n",
        "            # encode output sequence\n",
        "            if one_hot:\n",
        "              out_seq = to_categorical(out_seq, num_classes=len(self.word2index))\n",
        "   \n",
        "            \n",
        "            #print (len(in_seq),len(out_seq),out_seq.shape)\n",
        "            # store\n",
        "            X1.append(self.embeddings[file][num])\n",
        "            X2.append(in_seq)\n",
        "            y.append(out_seq)\n",
        "        if one_hot:\n",
        "          batch_y= np.array(y, dtype=np.int8)\n",
        "        else:\n",
        "          batch_y= np.array(y, dtype=np.int32)[:,:, np.newaxis] #create last axis as 1\n",
        "        return [[np.array(X1), np.array(X2)], batch_y]\n",
        "\n",
        "    def data_generator(self, file, data, batch_size,max_sent_len=None,one_hot=True):\n",
        "        \"\"\" max_sent_len should be below model allowed size.\"\"\"\n",
        "        while True:\n",
        "            batch = random.sample(range(*data), k=min(batch_size, len(range(*data))))\n",
        "            yield self.create_sequences(file, batch,max_sent_len,one_hot)\n",
        "\n",
        "    def normalize(self, sentence):\n",
        "        sentence = super().normalize(sentence)\n",
        "        psalms = re.findall('psalm [0-9]+', sentence)\n",
        "        if len(psalms) > 0:\n",
        "            sentence = sentence.split(psalms[0])[0]\n",
        "        sentence = re.sub(r'\\{(.*?)\\}', '', sentence)\n",
        "        sentence = re.sub('[0-9]+ <COLON> [0-9]+', '', sentence)\n",
        "\n",
        "        return sentence\n",
        "\n",
        "    def cluster(self, iteration):\n",
        "        clusters = {i: [] for i in range(self.max_sentence_length // iteration + 1)}\n",
        "        for style_index, style in self.index2style.items():\n",
        "            for sent_index, sent in enumerate(self.corpora[style]):\n",
        "                cluster = len(sent) // iteration\n",
        "                clusters[cluster].append((style_index, sent_index))\n",
        "\n",
        "        clusters = {k: (max([len(self.corpora[self.index2style[s[0]]][s[1]]) for s in v]), v) for k, v in\n",
        "                    clusters.items()}\n",
        "\n",
        "        return clusters\n",
        "   \n",
        "  \n",
        "  \n",
        " \n",
        "# iterate over the whole length of the sequence\n",
        "# currently done in batch=1, but can be done better...\n",
        "def test(model,sent=3,jump=2000):\n",
        "  for snum in range(0,sent*jump,jump):\n",
        "      \n",
        "      for file in ['<bbe>', '<ylt>']:\n",
        "          in_text = [dataset.word2index[START_SYMBOL]]\n",
        "          for i in range(dataset.max_sentence_length + 2):\n",
        "              # pad input\n",
        "              sequence = dataset.pad_sentence(in_text, dataset.max_sentence_length + 2)\n",
        "              # predict next word-seq. in practice we only take the i-th one\n",
        "              yhat = model.predict([dataset.embeddings[file][snum].reshape(1, -1), np.array(sequence).reshape(1, -1)],\n",
        "                                   verbose=0)\n",
        "              #print (yhat.shape) #1x62x3626\n",
        "              # convert probability to integer\n",
        "              SENT_IN_BATCH=0\n",
        "              yhat = np.argmax(yhat[SENT_IN_BATCH][i])  # FIX THIS:\n",
        "              #print (yhat, np.array(sequence).reshape(1, -1).shape,sequence)\n",
        "              # map integer to word\n",
        "              in_text += [yhat]\n",
        "              # stop if we predict the end of the sequence\n",
        "              if yhat == dataset.word2index[END_SYMBOL]:\n",
        "                  break\n",
        "          print('{}({}): {}'.format(snum, file, dataset.recostruct_sentence(in_text)))\n",
        "\n",
        "#test(model,10)\n",
        "  \n",
        "\n",
        "      \n",
        "dataset = BibleDataset([\"bbe\", \"ylt\"], \"embeddings.h5\", URL_ROOT, CSV_EXT)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "olVcmQ7MCAxF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load and test the models"
      ]
    },
    {
      "metadata": {
        "id": "du4Ur3wrtp7w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "3893891b-f046-4630-bcaf-8173ca94f0e5"
      },
      "cell_type": "code",
      "source": [
        "for (drive_id,desc) in [('1IKHJOeqkOzMEG3rSVJLFOhIky7ynSqk6','lstm0.763'),('1ofsHOPrxKtldw0-LgRhC_KnZ_0elybt9','tcn0.38')] :#best 2 models, TCN and LSTM\n",
        "  loaded_file_name = drive_id +'.h5'\n",
        "  print ('loading ',desc)\n",
        "  #drive.load_from_drive(drive_id,loaded_file_name)\n",
        "  download_file_from_google_drive(drive_id,loaded_file_name)\n",
        "  #print (model.summary())\n",
        "  \n",
        "  model = keras.models.load_model(loaded_file_name)\n",
        "  test(model,5,jump=1) \n",
        "  print ('')\n",
        " "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading  lstm0.763\n",
            "0(<bbe>): <start> at the first place god made , the earth <OOV> . <end>\n",
            "0(<ylt>): <start> in the beginning of god 's <OOV> the heavens and the earth <end>\n",
            "1(<bbe>): <start> and the dove is <OOV> from the waters , and it is <OOV> on the face of the waters , from under the waters of the earth . <end>\n",
            "1(<ylt>): <start> the earth hath <OOV> waste and void , and darkness is ' on the face of the waters , and on the waters of the deep , and <OOV> on the face of the waters , <end>\n",
            "2(<bbe>): <start> and jesus said , let light , be called , let me be hidden . <end>\n",
            "2(<ylt>): <start> and god saith , let light be ; ' and light is ' light . <end>\n",
            "3(<bbe>): <start> and jesus , having looked over the light , the light , and the <OOV> of god was seen , <end>\n",
            "3(<ylt>): <start> and god seeth the light that it is ' good , and the god of god is ' hard , <end>\n",
            "4(<bbe>): <start> teaching darkness , the night was the morning , and evening , and <OOV> , and evening , and <OOV> , and <OOV> . <end>\n",
            "4(<ylt>): <start> and god calleth to the light day , and there is an evening , and there is a morning day second . <end>\n",
            "\n",
            "loading  tcn0.38\n",
            "0(<bbe>): <start> at the first god made the earth . <end>\n",
            "0(<ylt>): <start> in the beginning of the reign of god 's prepared it is prepared in the heavens <end>\n",
            "1(<bbe>): <start> and was <OOV> on the earth , and the waters of <OOV> was on the face of the deep , and the spirit of the earth is moved . <end>\n",
            "1(<ylt>): <start> the spirit hath <OOV> on god , and the waters of deep is ' on the earth , and the waters <OOV> into the face of the deep , <end>\n",
            "2(<bbe>): <start> and there was there light , jesus , saying , god . <end>\n",
            "2(<ylt>): <start> and god saith , let light be ; ' and light is . <end>\n",
            "3(<bbe>): <start> and god was placed on the intelligent , and it <OOV> the <OOV> , and the light , that was on , <end>\n",
            "3(<ylt>): <start> and god seeth the light that it is ' good , and god <OOV> between the light and the darkness , <end>\n",
            "4(<bbe>): <start> evening , the light , and the night , and the continual evening . and evening was the morning , and there was no authority . <end>\n",
            "4(<ylt>): <start> and god calleth to the light day , and to night , and to night , and to night ; and evening having heat , there is a light to the morning . <end>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}